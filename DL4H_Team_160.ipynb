{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary. -->"
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU -->"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CS598-Project-Team-160-DeepMSPeptide**\n",
        "\n",
        "by Guangyao GUO gg39\n",
        "\n",
        "view at colab\n",
        "https://colab.research.google.com/drive/1rUu91UYnTde-eOlNTz9LHtBqQ09TuQM_?usp=sharing\n",
        "\n",
        "and github\n",
        "https://github.com/guangyaoguo/DL4H_Team_160.git\n",
        "\n",
        "video presentation at Youtube\n",
        "https://youtu.be/NOoaBaST0Ak\n"
      ],
      "metadata": {
        "id": "ZR0vmy2Zek-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fFwV3aULJTga",
        "outputId": "f32951c8-ba47-4266-a09b-86aedc87bb56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # Introduction\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "  * what is the importance/meaning of solving the problem\n",
        "  * what is the difficulty of the problem\n",
        "  * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "  * what did the paper propose\n",
        "  * what is the innovations of the method\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem). -->\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This introduction provides an overview of the challenges and innovations presented in the paper concerning peptide detectability prediction using mass spectrometry in proteomics.\n",
        "\n",
        "##   Background of the Problem\n",
        "  * In proteomics, accurately predicting peptide detectability via mass spectrometry is crucial for enhancing experiment design and data interpretation. The challenges lie in the complexity of peptide sequences and limitations of mass spectrometry technologies. Addressing this problem could significantly improve proteomic studies crucial for disease diagnostics, biomarker discovery, and biological research.\n",
        "\n",
        "  * Traditional methods for peptide detectability prediction rely on empirical physicochemical properties such as hydrophobicity and molecular weight. These methods require extensive preprocessing and do not always capture the nuances of peptide behavior in mass spectrometry, limiting their effectiveness and scalability.\n",
        "\n",
        "##   Paper Explanation\n",
        "  * The paper introduces a 1D Convolutional Neural Network (1D-2C-CNN) that predicts peptide detectability based solely on amino acid sequences. This deep learning approach allows the model to automatically learn complex patterns in sequence data without the need for manual feature engineering, representing a significant shift from traditional methods.\n",
        "\n",
        "  * The 1D-2C-CNN demonstrated high accuracy and precision, outperforming traditional models and enhancing metrics such as accuracy and recall. Its main contribution is the introduction of a scalable and automated deep learning framework to the field of proteomics, potentially transforming peptide analysis.\n",
        "\n",
        "  * This research addresses critical limitations in proteomic analysis by offering a scalable, accurate method for peptide detectability prediction. It contributes significantly to proteomics, improving the quality and efficiency of experiments and advancing our understanding of biological systems and diseases."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # Scope of Reproducibility:\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: xxxxxxx\n",
        "2.   Hypothesis 2: xxxxxxx\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc) -->\n",
        "\n",
        "# Scope of Reproducibility\n",
        "\n",
        "The goal of this reproducibility effort is to validate key hypotheses from the original study on the 1D Convolutional Neural Network (1D-2C-CNN) for peptide detectability prediction. We aim to verify the model's reported performance and assess its generalizability and practical application in proteomics. Here are the hypotheses we plan to test, alongside the experiments we will conduct:\n",
        "\n",
        "## Hypotheses and Experiments\n",
        "\n",
        "1. **Hypothesis 1: The 1D-2C-CNN model can predict peptide detectability with higher accuracy and precision than traditional methods that rely on physicochemical properties.**\n",
        "   - **Experiment 1**: We will train the 1D-2C-CNN model using the same dataset as used in the original study and compare its performance metrics (accuracy, precision, recall) against those reported for traditional models.\n",
        "\n",
        "2. **Hypothesis 2: The performance of the 1D-2C-CNN model, as demonstrated in the paper, is most effective when it relies solely on amino acid sequences, without the need for complex physicochemical properties.\n",
        ".**\n",
        "   - **Experiment 2**: We will determine whether the 1D-2C-CNN model can achieve highest precision in predicting the detectability of proteotypic peptides using only their amino acid sequences.\n",
        "\n",
        "3. **Hypothesis 3: The 1D-2C-CNN model's performance is scalable and maintains high efficiency even when computational resources are limited.**\n",
        "   - **Experiment 3**: We will measure the computational resources (time and memory usage) required to train and deploy the model under constrained settings to verify scalability and efficiency as claimed.\n",
        "\n",
        "These experiments will help us determine the validity of the model's capabilities and whether the results from the original study can be reliably reproduced in different settings. This will also provide insight into the practical implications of deploying such a model in real-world proteomic analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- # Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment. -->\n",
        "# Methodology\n",
        "\n",
        "## Environment\n",
        "\n",
        "To ensure reproducibility and consistent performance across different machines, the following software environment and packages were used in the development and execution of the model:\n",
        "\n",
        "### Python Version\n",
        "\n",
        "The experiments were conducted using **Python 3.8**. This version is recommended due to its stability and support for the libraries used in the project.\n",
        "\n",
        "### Dependencies and Packages\n",
        "\n",
        "The model relies on several third-party libraries which need to be installed. Below are the key dependencies along with their tested versions:\n",
        "\n",
        "- **numpy** (1.19.5): Used for numerical operations.\n",
        "- **pandas** (1.2.0): Used for data manipulation and analysis.\n",
        "- **tensorflow** (2.4.0): Provides the backend for constructing and training the deep learning models.\n",
        "- **scikit-learn** (0.24.0): Used for data preprocessing, specifically for encoding labels and splitting the dataset.\n",
        "\n",
        "### Installation\n",
        "\n",
        "To set up the environment, you can use `pip` to install the required packages. Here is an example command to install all necessary libraries:\n",
        "\n",
        "```bash\n",
        "pip install numpy==1.19.5 pandas==1.2.0 tensorflow==2.4.0 scikit-learn==0.24.0\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DpNwJbc9Moyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, Activation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ##  Data\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset. -->\n",
        "## Data\n",
        "\n",
        "The dataset used in this study is derived from the Global Proteome Machine Database (GPMDB), which collects peptide sequences detected in various mass spectrometry (MS/MS) experiments. This dataset includes descriptive statistics about the detection frequencies of peptides, essential for the study of peptide detectability via deep learning.\n",
        "## Data\n",
        "The data used in this study consists of peptide sequences along with their detectability status obtained from mass spectrometry experiments. Each sequence is labeled as detectable or not based on empirical evidence.\n",
        "\n",
        "\n",
        "###Data Descriptions\n",
        "The dataset includes the following features:\n",
        "\n",
        "Peptide_seq: Amino acid sequences of peptides.\n",
        "Class: Binary labels indicating whether a peptide is detectable (1) or not (0).\n",
        "\n",
        "### Source of the Data\n",
        "\n",
        "The data was extracted from the GPMDB database. This database compiles findings from hundreds of MS/MS experiments, providing a rich source of peptides and their detection statuses. The dataset used is synthetic in nature, derived from real experimental data to form a comprehensive peptide detection dataset. The raw dataset can be accessed via the GPMDB website, although for confidentiality and size reasons, direct links to the data are typically restricted to academic users.\n",
        "\n",
        "### Statistics\n",
        "\n",
        "The dataset includes thousands of peptides with the following descriptive statistics:\n",
        "- **Size**: Approximately 74997*2 peptide sequences.\n",
        "- **Cross-Validation Split**: 40% training, 10% validation, and 50% test set.\n",
        "- **Label Distribution**: About 60% of the peptides are labeled as detectable (`1`) and 40% as not detectable (`0`).\n",
        "\n",
        "\n",
        "### Data download instruction\n",
        "The data is originally from Global Proteome Machine Database (GPMDB) https://www.thegpm.org/gpmdb/index.html, however, for easier access, you could download from the github of the original paper https://github.com/vsegurar/DeepMSPeptide/tree/master/Datasets.\n",
        "\n",
        "### Data Pre-Processing\n",
        "\n",
        "The data was preprocessed as follows:\n",
        "- **Tokenization**: Peptide sequences were tokenized into individual amino acids.\n",
        "- **Integer Encoding**: Each amino acid in a peptide sequence was encoded as an integer.\n",
        "- **Padding**: Sequences were padded to ensure a uniform length of 81 characters, which corresponds to the longest sequence in the dataset.\n",
        "- **Normalization**: No normalization was required as the data consists of categorical sequence information.\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pep_and_codify(lines, max_len):\n",
        "    aa_dict={'A':1,'R':2,'N':3,'D':4,'C':5,'Q':6,'E':7,'G':8,'H':9,'I':10,'L':11,'K':12,'M':13,'F':14,\n",
        "        'P':15,'O':16,'S':17,'U':18,'T':19,'W':20,'Y':21,'V':22}\n",
        "    # with open(file, 'r') as inf:\n",
        "    #     lines = inf.read().splitlines()\n",
        "    pep_codes=[]\n",
        "    long_pep_counter = 0\n",
        "    newLines = []\n",
        "    print(lines[0])\n",
        "    for pep in lines:\n",
        "        if not len(pep) > max_len:\n",
        "            current_pep=[]\n",
        "            for aa in pep:\n",
        "                current_pep.append(aa_dict[aa])\n",
        "            pep_codes.append(current_pep)\n",
        "            newLines.extend([pep])\n",
        "        else:\n",
        "            long_pep_counter += 1\n",
        "    predict_data = keras.preprocessing.sequence.pad_sequences(pep_codes, value=0, padding='post', maxlen=max_len)\n",
        "    return predict_data, long_pep_counter, newLines"
      ],
      "metadata": {
        "id": "GYWSgGY-9pOe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BZScZNbROw-N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "27e4ff40-a532-416e-e33b-451be0680c0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Peptide_seq  BUNA790103  CHAM820102  CHOP780207  \\\n",
              "0              AMGIMNSFVNDIFER    6.286667    0.453200    0.962667   \n",
              "1                IQDKEGIPPDQQR    5.538462    0.279231    1.083077   \n",
              "2               LHFFMPGFAPLTSR    5.828571   -0.067929    1.109286   \n",
              "3             SYELPDGQVITIGNER    6.268750    0.590625    1.020625   \n",
              "4                    TAVCDIPPR    5.444444    0.211556    0.996667   \n",
              "...                        ...         ...         ...         ...   \n",
              "74993       VPPGLFNVVQGGAATGQF    6.005556   -0.081167    1.030000   \n",
              "74994         FIVVSMLSAIRGFSLE    6.581250    0.348438    0.840000   \n",
              "74995  FDPTWESLDARQLPAWFDQAKFG    5.786957    0.446478    0.920435   \n",
              "74996               VDGYIWSWTK    5.330000    1.031300    0.845000   \n",
              "74997           SQKLSPIYNLVPVK    5.735714    0.274714    1.035000   \n",
              "\n",
              "       FASG760102  FASG760103  GEIM800103  KARP850103  KHAG800101  LEWP710101  \\\n",
              "0      269.266667   -3.061333    1.046000    0.897133   31.613333    0.314667   \n",
              "1      239.076923   -6.115385    1.008462    0.918538   29.461538    0.370000   \n",
              "2      274.000000  -26.257143    0.757143    0.908500   49.107143    0.298571   \n",
              "3      265.750000   -4.376250    0.992500    0.910250   30.043750    0.355000   \n",
              "4      250.777778  -19.946667    0.846667    0.914778   38.788889    0.348889   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "74993  268.944444  -14.050556    0.770000    0.914556   40.283333    0.316667   \n",
              "74994  277.312500   -3.821250    0.970625    0.903875   39.006250    0.270625   \n",
              "74995  264.652174  -14.284783    1.070435    0.911609   38.443478    0.348696   \n",
              "74996  275.000000   -7.522000    1.161000    0.899700   32.940000    0.424000   \n",
              "74997  261.214286  -11.845714    0.950000    0.929714   21.564286    0.318571   \n",
              "\n",
              "       ...  ZASB820101  YUTK870104  AVBF000106  Aliphatic     Basic    Acidic  \\\n",
              "0      ...   -0.154333   17.190667   -0.238133   0.200000  0.066667  0.133333   \n",
              "1      ...   -0.195308   17.097692   -0.253909   0.153846  0.153846  0.230769   \n",
              "2      ...   -0.105286   17.102857   -0.274917   0.142857  0.142857  0.000000   \n",
              "3      ...   -0.186438   17.309375   -0.238400   0.250000  0.062500  0.187500   \n",
              "4      ...   -0.148778   16.420000   -0.196714   0.222222  0.111111  0.111111   \n",
              "...    ...         ...         ...         ...        ...       ...       ...   \n",
              "74993  ...   -0.146444   18.542222   -0.300375   0.222222  0.000000  0.000000   \n",
              "74994  ...   -0.136375   17.379375   -0.239187   0.375000  0.062500  0.062500   \n",
              "74995  ...   -0.131348   17.406087   -0.252143   0.086957  0.086957  0.173913   \n",
              "74996  ...   -0.064100   18.080000   -0.209300   0.200000  0.100000  0.100000   \n",
              "74997  ...   -0.129714   18.610714   -0.224000   0.357143  0.142857  0.000000   \n",
              "\n",
              "       pep_length    pep_PI     pep_PMW  Class  \n",
              "0              15  4.370356  1744.00032   MObs  \n",
              "1              13  4.558133  1523.64664   MObs  \n",
              "2              14  9.756550  1620.91392   MObs  \n",
              "3              16  4.136799  1790.92442   MObs  \n",
              "4               9  5.496877   971.13210   MObs  \n",
              "...           ...       ...         ...    ...  \n",
              "74993          18  5.494998  1758.97022   LObs  \n",
              "74994          16  6.218211  1769.11252   LObs  \n",
              "74995          23  4.225967  2725.96108   LObs  \n",
              "74996          10  5.805261  1254.38922   LObs  \n",
              "74997          14  9.700074  1585.88496   LObs  \n",
              "\n",
              "[74998 rows x 59 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac391d7f-cc5e-49d4-8338-6de0634cfc51\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Peptide_seq</th>\n",
              "      <th>BUNA790103</th>\n",
              "      <th>CHAM820102</th>\n",
              "      <th>CHOP780207</th>\n",
              "      <th>FASG760102</th>\n",
              "      <th>FASG760103</th>\n",
              "      <th>GEIM800103</th>\n",
              "      <th>KARP850103</th>\n",
              "      <th>KHAG800101</th>\n",
              "      <th>LEWP710101</th>\n",
              "      <th>...</th>\n",
              "      <th>ZASB820101</th>\n",
              "      <th>YUTK870104</th>\n",
              "      <th>AVBF000106</th>\n",
              "      <th>Aliphatic</th>\n",
              "      <th>Basic</th>\n",
              "      <th>Acidic</th>\n",
              "      <th>pep_length</th>\n",
              "      <th>pep_PI</th>\n",
              "      <th>pep_PMW</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AMGIMNSFVNDIFER</td>\n",
              "      <td>6.286667</td>\n",
              "      <td>0.453200</td>\n",
              "      <td>0.962667</td>\n",
              "      <td>269.266667</td>\n",
              "      <td>-3.061333</td>\n",
              "      <td>1.046000</td>\n",
              "      <td>0.897133</td>\n",
              "      <td>31.613333</td>\n",
              "      <td>0.314667</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.154333</td>\n",
              "      <td>17.190667</td>\n",
              "      <td>-0.238133</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>15</td>\n",
              "      <td>4.370356</td>\n",
              "      <td>1744.00032</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IQDKEGIPPDQQR</td>\n",
              "      <td>5.538462</td>\n",
              "      <td>0.279231</td>\n",
              "      <td>1.083077</td>\n",
              "      <td>239.076923</td>\n",
              "      <td>-6.115385</td>\n",
              "      <td>1.008462</td>\n",
              "      <td>0.918538</td>\n",
              "      <td>29.461538</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195308</td>\n",
              "      <td>17.097692</td>\n",
              "      <td>-0.253909</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>13</td>\n",
              "      <td>4.558133</td>\n",
              "      <td>1523.64664</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LHFFMPGFAPLTSR</td>\n",
              "      <td>5.828571</td>\n",
              "      <td>-0.067929</td>\n",
              "      <td>1.109286</td>\n",
              "      <td>274.000000</td>\n",
              "      <td>-26.257143</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.908500</td>\n",
              "      <td>49.107143</td>\n",
              "      <td>0.298571</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.105286</td>\n",
              "      <td>17.102857</td>\n",
              "      <td>-0.274917</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>9.756550</td>\n",
              "      <td>1620.91392</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SYELPDGQVITIGNER</td>\n",
              "      <td>6.268750</td>\n",
              "      <td>0.590625</td>\n",
              "      <td>1.020625</td>\n",
              "      <td>265.750000</td>\n",
              "      <td>-4.376250</td>\n",
              "      <td>0.992500</td>\n",
              "      <td>0.910250</td>\n",
              "      <td>30.043750</td>\n",
              "      <td>0.355000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.186438</td>\n",
              "      <td>17.309375</td>\n",
              "      <td>-0.238400</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>16</td>\n",
              "      <td>4.136799</td>\n",
              "      <td>1790.92442</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TAVCDIPPR</td>\n",
              "      <td>5.444444</td>\n",
              "      <td>0.211556</td>\n",
              "      <td>0.996667</td>\n",
              "      <td>250.777778</td>\n",
              "      <td>-19.946667</td>\n",
              "      <td>0.846667</td>\n",
              "      <td>0.914778</td>\n",
              "      <td>38.788889</td>\n",
              "      <td>0.348889</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.148778</td>\n",
              "      <td>16.420000</td>\n",
              "      <td>-0.196714</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>9</td>\n",
              "      <td>5.496877</td>\n",
              "      <td>971.13210</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74993</th>\n",
              "      <td>VPPGLFNVVQGGAATGQF</td>\n",
              "      <td>6.005556</td>\n",
              "      <td>-0.081167</td>\n",
              "      <td>1.030000</td>\n",
              "      <td>268.944444</td>\n",
              "      <td>-14.050556</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.914556</td>\n",
              "      <td>40.283333</td>\n",
              "      <td>0.316667</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.146444</td>\n",
              "      <td>18.542222</td>\n",
              "      <td>-0.300375</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18</td>\n",
              "      <td>5.494998</td>\n",
              "      <td>1758.97022</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74994</th>\n",
              "      <td>FIVVSMLSAIRGFSLE</td>\n",
              "      <td>6.581250</td>\n",
              "      <td>0.348438</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>277.312500</td>\n",
              "      <td>-3.821250</td>\n",
              "      <td>0.970625</td>\n",
              "      <td>0.903875</td>\n",
              "      <td>39.006250</td>\n",
              "      <td>0.270625</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.136375</td>\n",
              "      <td>17.379375</td>\n",
              "      <td>-0.239187</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>16</td>\n",
              "      <td>6.218211</td>\n",
              "      <td>1769.11252</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74995</th>\n",
              "      <td>FDPTWESLDARQLPAWFDQAKFG</td>\n",
              "      <td>5.786957</td>\n",
              "      <td>0.446478</td>\n",
              "      <td>0.920435</td>\n",
              "      <td>264.652174</td>\n",
              "      <td>-14.284783</td>\n",
              "      <td>1.070435</td>\n",
              "      <td>0.911609</td>\n",
              "      <td>38.443478</td>\n",
              "      <td>0.348696</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.131348</td>\n",
              "      <td>17.406087</td>\n",
              "      <td>-0.252143</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>23</td>\n",
              "      <td>4.225967</td>\n",
              "      <td>2725.96108</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74996</th>\n",
              "      <td>VDGYIWSWTK</td>\n",
              "      <td>5.330000</td>\n",
              "      <td>1.031300</td>\n",
              "      <td>0.845000</td>\n",
              "      <td>275.000000</td>\n",
              "      <td>-7.522000</td>\n",
              "      <td>1.161000</td>\n",
              "      <td>0.899700</td>\n",
              "      <td>32.940000</td>\n",
              "      <td>0.424000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064100</td>\n",
              "      <td>18.080000</td>\n",
              "      <td>-0.209300</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>10</td>\n",
              "      <td>5.805261</td>\n",
              "      <td>1254.38922</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74997</th>\n",
              "      <td>SQKLSPIYNLVPVK</td>\n",
              "      <td>5.735714</td>\n",
              "      <td>0.274714</td>\n",
              "      <td>1.035000</td>\n",
              "      <td>261.214286</td>\n",
              "      <td>-11.845714</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.929714</td>\n",
              "      <td>21.564286</td>\n",
              "      <td>0.318571</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129714</td>\n",
              "      <td>18.610714</td>\n",
              "      <td>-0.224000</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>9.700074</td>\n",
              "      <td>1585.88496</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>74998 rows Ã— 59 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac391d7f-cc5e-49d4-8338-6de0634cfc51')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac391d7f-cc5e-49d4-8338-6de0634cfc51 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac391d7f-cc5e-49d4-8338-6de0634cfc51');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-861f2bf7-7505-4f67-aa58-fcaba8e87966\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-861f2bf7-7505-4f67-aa58-fcaba8e87966')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-861f2bf7-7505-4f67-aa58-fcaba8e87966 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "processed_data"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = '/content/drive/MyDrive/DL4H_project/datasets/GPMDB_training_peptides.txt'\n",
        "sample_input_data_dir = '/content/drive/MyDrive/DL4H_project/datasets/inputExample.txt'\n",
        "test_data_dir = '/content/drive/MyDrive/DL4H_project/datasets/GPMDB_test_peptides.txt'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  return None\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  return None\n",
        "\n",
        "# process raw data\n",
        "def process_data(raw_data):\n",
        "    # implement this function to process the data as you need\n",
        "    # Load the data\n",
        "    data = pd.read_csv(raw_data_dir, sep='\\t')\n",
        "\n",
        "    # Basic preprocessing\n",
        "    # Encoding categorical data (peptide sequences)\n",
        "    # predict_data, skipped,  lines = load_pep_and_codify(data['Peptide_seq'], 81)\n",
        "    # data['ptide_seq_encoded'] = predict_data\n",
        "\n",
        "    return data\n",
        "\n",
        "processed_data = process_data(raw_data)\n",
        "\n",
        "''' you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''\n",
        "processed_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embd, skipped,  lines = load_pep_and_codify(processed_data['Peptide_seq'], 81)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ls9PryyS9vZ6",
        "outputId": "578da284-5988-42a8-9cf6-3f51bba7950a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AMGIMNSFVNDIFER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RiUbz5Q3-RTd",
        "outputId": "06d05e00-e2ea-4bc4-dac0-78a1809c0fa7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1, 13,  8, ...,  0,  0,  0],\n",
              "       [10,  6,  4, ...,  0,  0,  0],\n",
              "       [11,  9, 14, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [14,  4, 15, ...,  0,  0,  0],\n",
              "       [22,  4,  8, ...,  0,  0,  0],\n",
              "       [17,  6, 12, ...,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = embd\n",
        "\n",
        "# turn class label to numerical\n",
        "y = processed_data['Class'].apply(lambda x: 1 if x == 'MObs' else 0).values"
      ],
      "metadata": {
        "id": "du69fBnjERz6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zyQQaWAEEuzh",
        "outputId": "aba0ff4e-e45b-435c-ad0b-c28d74f07f26"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ##   Model\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it. -->\n",
        "\n",
        "## Model\n",
        "\n",
        "The model described in this section is specifically designed for predicting peptide detectability based on their amino acid sequences using a deep learning framework, specifically a 1D Convolutional Neural Network (1D-2C-CNN).\n",
        "\n",
        "### Citation to the original paper\n",
        "This model is conceived by Serrano, G., Guruceaga, E., & Segura, V. (2020). DeepMSPeptide: peptide detectability prediction using deep learning. Bioinformatics (Oxford, England), 36(4), 1279â€“1280. https://doi.org/10.1093/bioinformatics/btz708\n",
        "\n",
        "However, the specific code of model is not provided, therefore we do not add citation in code.\n",
        "\n",
        "### Link to the original paperâ€™s repo\n",
        "https://github.com/vsegurar/DeepMSPeptide\n",
        "\n",
        "### Model Descriptions\n",
        "The model is a 1D-2C-CNN which includes:\n",
        "\n",
        "An Embedding layer that converts integer-encoded amino acids into dense vectors.\n",
        "Two Conv1D layers with ReLU activation to capture spatial hierarchy in sequences.\n",
        "A GlobalMaxPooling1D layer to reduce dimensionality.\n",
        "A Dense layer followed by a Dropout layer for regularization.\n",
        "A final Dense layer with a sigmoid activation function for binary classification.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "The architecture of the model includes several layers designed to process sequence data effectively:\n",
        "\n",
        "- **Embedding Layer**: Converts integer-encoded amino acids into dense vectors of size 50. The input dimension is set to 23 (22 unique amino acids plus one for padding), with an input length of 81 corresponding to the maximum sequence length.\n",
        "- **Convolutional Layers**: Two convolutional layers, each with a kernel size of 3 and using the 'relu' activation function. The first layer features 128 filters, while the second layer has 64 filters, both using 'same' padding to preserve sequence length.\n",
        "- **Global Max Pooling Layer**: Reduces the dimensionality of the data by taking the maximum value over the time dimension, which helps in capturing the most significant features from the convolutional layers.\n",
        "- **Dropout Layer**: Included with a rate of 0.5 to prevent overfitting by randomly setting input units to 0 during training at each update.\n",
        "- **Dense Layers**: A dense layer with 64 units followed by a relu activation function to further process features, leading to the final dense layer with a single unit and a sigmoid activation function for binary classification."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "input_length = 81  # Length of input sequences\n",
        "vocab_size = 22    # Assuming there are 23 unique amino acids (you might need to adjust this based on the data)\n",
        "\n",
        "filter_options = [32, 128, 256]\n",
        "kernel_sizes = [3, 5, 7]\n",
        "dropout_rates = [0.3, 0.5]\n",
        "\n",
        "model_list = []\n",
        "# Loop over filter options\n",
        "for filters in filter_options:\n",
        "    # Loop over kernel size options\n",
        "    for kernel_size in kernel_sizes:\n",
        "        # Loop over dropout rate options\n",
        "        for dropout_rate in dropout_rates:\n",
        "            model = Sequential([\n",
        "                Embedding(input_dim=vocab_size + 1, output_dim=50, input_length=input_length),\n",
        "                Conv1D(filters, kernel_size, padding='same', activation='relu'),\n",
        "                Conv1D(filters // 2, kernel_size, padding='same', activation='relu'),\n",
        "                GlobalMaxPooling1D(),\n",
        "                Dropout(dropout_rate),\n",
        "                Dense(64, activation='relu'),\n",
        "                Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "\n",
        "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "            model_list.append(model)\n",
        "            # Summary to see the configured model\n",
        "\n",
        "# Model summary to see the architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kaYF5vbJC_pM",
        "outputId": "11bc1b7f-5e52-4c8e-ce8c-dfb8023d7a99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_31 (Embedding)    (None, 81, 50)            1150      \n",
            "                                                                 \n",
            " conv1d_62 (Conv1D)          (None, 81, 256)           89856     \n",
            "                                                                 \n",
            " conv1d_63 (Conv1D)          (None, 81, 128)           229504    \n",
            "                                                                 \n",
            " global_max_pooling1d_31 (G  (None, 128)               0         \n",
            " lobalMaxPooling1D)                                              \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 328831 (1.25 MB)\n",
            "Trainable params: 328831 (1.25 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "### Hyperparams\n",
        "\n",
        "In addition to vanilla version, we further implement a hyperparameters tuning process for the model, specifically include 3 parameters: filter size, kernel size and dropout rate. It significantly improved the performance of our model, and we will show it later.\n",
        "\n",
        "### Computational Requirements\n",
        "\n",
        "- **Hardware** The model training was executed on Colab, specifically on a remote machine with Intel(R) Xeon(R) CPU @ 2.20GHz, T4 GPU and 12.7GB RAM which provided sufficient computational power to handle the model's requirements due to it's lightweight.\n",
        "\n",
        "- **Runtime** Training took approximately 2 minutes to complete 10 epochs(avg 11s for each spoch) for each model,demonstrating that the model is relatively efficient and can be trained on moderately equipped hardware. For the hyperparameter tuning, we have 18 model in total therefore we need around 36 minutes to tune the model. We choose to train each model with 10 epoch as we found that the convergence rate of accuracy and loss is significantly slow approaching 10 epochs."
      ],
      "metadata": {
        "id": "jWAkCzsJYhIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lgRtQq9owEhi",
        "outputId": "feed7787-d70d-4336-b5f2-65d1cf1ed559"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0xffffffff\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XgNbPtJLwsrQ",
        "outputId": "5a57953d-4372-4e4d-aec8-9b6a47c5fd99"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13290480 kB\n",
            "MemFree:         4123148 kB\n",
            "MemAvailable:    9122588 kB\n",
            "Buffers:          365716 kB\n",
            "Cached:          4830984 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           920156 kB\n",
            "Inactive:        7805176 kB\n",
            "Active(anon):       2036 kB\n",
            "Inactive(anon):  3545196 kB\n",
            "Active(file):     918120 kB\n",
            "Inactive(file):  4259980 kB\n",
            "Unevictable:          16 kB\n",
            "Mlocked:              16 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               340 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:       3526404 kB\n",
            "Mapped:           949104 kB\n",
            "Shmem:             18592 kB\n",
            "KReclaimable:     124556 kB\n",
            "Slab:             175904 kB\n",
            "SReclaimable:     124556 kB\n",
            "SUnreclaim:        51348 kB\n",
            "KernelStack:        7088 kB\n",
            "PageTables:        32012 kB\n",
            "SecPageTables:         0 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6645240 kB\n",
            "Committed_AS:    5479180 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       81444 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1088 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:    555008 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "Unaccepted:            0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      246584 kB\n",
            "DirectMap2M:     6041600 kB\n",
            "DirectMap1G:     9437184 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in model_list:\n",
        "  m.fit(x, y, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ra4LB0OmEzPX",
        "outputId": "bc0b4803-a831-49ca-df8c-56a5f8f6928d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 17s 6ms/step - loss: 0.5005 - accuracy: 0.7803 - val_loss: 0.6083 - val_accuracy: 0.6515\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4693 - accuracy: 0.8001 - val_loss: 0.6296 - val_accuracy: 0.6499\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4638 - accuracy: 0.8014 - val_loss: 0.6635 - val_accuracy: 0.6373\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4608 - accuracy: 0.8045 - val_loss: 0.7369 - val_accuracy: 0.6205\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4586 - accuracy: 0.8069 - val_loss: 0.5406 - val_accuracy: 0.6764\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4570 - accuracy: 0.8060 - val_loss: 0.6154 - val_accuracy: 0.6513\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4543 - accuracy: 0.8073 - val_loss: 0.6159 - val_accuracy: 0.6363\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4528 - accuracy: 0.8085 - val_loss: 0.6071 - val_accuracy: 0.6529\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4513 - accuracy: 0.8097 - val_loss: 0.5923 - val_accuracy: 0.6441\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4495 - accuracy: 0.8099 - val_loss: 0.5935 - val_accuracy: 0.6556\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.5288 - accuracy: 0.7541 - val_loss: 0.5274 - val_accuracy: 0.6613\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4918 - accuracy: 0.7855 - val_loss: 0.5115 - val_accuracy: 0.6881\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4829 - accuracy: 0.7910 - val_loss: 0.5543 - val_accuracy: 0.6715\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4769 - accuracy: 0.7960 - val_loss: 0.6096 - val_accuracy: 0.6320\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4769 - accuracy: 0.7945 - val_loss: 0.5348 - val_accuracy: 0.6818\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4759 - accuracy: 0.7951 - val_loss: 0.5118 - val_accuracy: 0.6771\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4734 - accuracy: 0.7967 - val_loss: 0.5658 - val_accuracy: 0.6340\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4713 - accuracy: 0.7976 - val_loss: 0.5106 - val_accuracy: 0.6806\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4707 - accuracy: 0.7988 - val_loss: 0.5590 - val_accuracy: 0.6377\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4679 - accuracy: 0.8011 - val_loss: 0.5427 - val_accuracy: 0.6558\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4995 - accuracy: 0.7777 - val_loss: 0.5497 - val_accuracy: 0.6571\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4626 - accuracy: 0.8038 - val_loss: 0.5358 - val_accuracy: 0.6815\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4548 - accuracy: 0.8073 - val_loss: 0.5578 - val_accuracy: 0.6704\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4504 - accuracy: 0.8083 - val_loss: 0.6559 - val_accuracy: 0.6457\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4467 - accuracy: 0.8100 - val_loss: 0.5195 - val_accuracy: 0.6881\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4421 - accuracy: 0.8130 - val_loss: 0.5456 - val_accuracy: 0.6687\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4401 - accuracy: 0.8143 - val_loss: 0.5545 - val_accuracy: 0.6768\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4384 - accuracy: 0.8158 - val_loss: 0.5312 - val_accuracy: 0.6940\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4357 - accuracy: 0.8176 - val_loss: 0.5476 - val_accuracy: 0.6865\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4329 - accuracy: 0.8168 - val_loss: 0.5678 - val_accuracy: 0.6783\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.5113 - accuracy: 0.7711 - val_loss: 0.4739 - val_accuracy: 0.6923\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4723 - accuracy: 0.8013 - val_loss: 0.5024 - val_accuracy: 0.6624\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4661 - accuracy: 0.8043 - val_loss: 0.5391 - val_accuracy: 0.6818\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4629 - accuracy: 0.8041 - val_loss: 0.5561 - val_accuracy: 0.6804\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4605 - accuracy: 0.8059 - val_loss: 0.6689 - val_accuracy: 0.6236\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4584 - accuracy: 0.8047 - val_loss: 0.6309 - val_accuracy: 0.6201\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4565 - accuracy: 0.8081 - val_loss: 0.5034 - val_accuracy: 0.6917\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4537 - accuracy: 0.8100 - val_loss: 0.6053 - val_accuracy: 0.6330\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4517 - accuracy: 0.8106 - val_loss: 0.5655 - val_accuracy: 0.6261\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4513 - accuracy: 0.8108 - val_loss: 0.5518 - val_accuracy: 0.6489\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.4939 - accuracy: 0.7817 - val_loss: 0.6016 - val_accuracy: 0.6580\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4585 - accuracy: 0.8061 - val_loss: 0.5290 - val_accuracy: 0.6835\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4500 - accuracy: 0.8104 - val_loss: 0.5735 - val_accuracy: 0.6925\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4465 - accuracy: 0.8121 - val_loss: 0.5463 - val_accuracy: 0.6915\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4419 - accuracy: 0.8139 - val_loss: 0.5311 - val_accuracy: 0.6940\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4387 - accuracy: 0.8150 - val_loss: 0.5782 - val_accuracy: 0.6558\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4332 - accuracy: 0.8195 - val_loss: 0.5121 - val_accuracy: 0.6825\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4297 - accuracy: 0.8211 - val_loss: 0.5405 - val_accuracy: 0.6721\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4255 - accuracy: 0.8209 - val_loss: 0.6000 - val_accuracy: 0.6603\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4234 - accuracy: 0.8225 - val_loss: 0.5128 - val_accuracy: 0.6781\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 12s 5ms/step - loss: 0.5282 - accuracy: 0.7562 - val_loss: 0.4142 - val_accuracy: 0.7137\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4756 - accuracy: 0.7971 - val_loss: 0.4948 - val_accuracy: 0.6871\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4689 - accuracy: 0.8002 - val_loss: 0.4580 - val_accuracy: 0.6979\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4631 - accuracy: 0.8034 - val_loss: 0.4805 - val_accuracy: 0.6998\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4591 - accuracy: 0.8051 - val_loss: 0.4450 - val_accuracy: 0.7079\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4553 - accuracy: 0.8068 - val_loss: 0.5183 - val_accuracy: 0.6714\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4545 - accuracy: 0.8079 - val_loss: 0.5043 - val_accuracy: 0.6820\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4498 - accuracy: 0.8102 - val_loss: 0.5155 - val_accuracy: 0.6747\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4452 - accuracy: 0.8124 - val_loss: 0.5010 - val_accuracy: 0.6943\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4425 - accuracy: 0.8136 - val_loss: 0.4706 - val_accuracy: 0.6921\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.4893 - accuracy: 0.7871 - val_loss: 0.4754 - val_accuracy: 0.6701\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4602 - accuracy: 0.8056 - val_loss: 0.5411 - val_accuracy: 0.6625\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4549 - accuracy: 0.8085 - val_loss: 0.5568 - val_accuracy: 0.6751\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4505 - accuracy: 0.8102 - val_loss: 0.5723 - val_accuracy: 0.6599\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4462 - accuracy: 0.8126 - val_loss: 0.5829 - val_accuracy: 0.6702\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4418 - accuracy: 0.8126 - val_loss: 0.6578 - val_accuracy: 0.6462\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4382 - accuracy: 0.8158 - val_loss: 0.5845 - val_accuracy: 0.6724\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4337 - accuracy: 0.8168 - val_loss: 0.5586 - val_accuracy: 0.6835\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4301 - accuracy: 0.8189 - val_loss: 0.5252 - val_accuracy: 0.6783\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4262 - accuracy: 0.8198 - val_loss: 0.5819 - val_accuracy: 0.6677\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4989 - accuracy: 0.7808 - val_loss: 0.6523 - val_accuracy: 0.6478\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4690 - accuracy: 0.8013 - val_loss: 0.7201 - val_accuracy: 0.6199\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4637 - accuracy: 0.8040 - val_loss: 0.6257 - val_accuracy: 0.6509\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4582 - accuracy: 0.8065 - val_loss: 0.5907 - val_accuracy: 0.6743\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4570 - accuracy: 0.8073 - val_loss: 0.5454 - val_accuracy: 0.6489\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4540 - accuracy: 0.8073 - val_loss: 0.6453 - val_accuracy: 0.6537\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4522 - accuracy: 0.8085 - val_loss: 0.7012 - val_accuracy: 0.6482\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4497 - accuracy: 0.8096 - val_loss: 0.6026 - val_accuracy: 0.6486\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4470 - accuracy: 0.8101 - val_loss: 0.6280 - val_accuracy: 0.6513\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4450 - accuracy: 0.8103 - val_loss: 0.6114 - val_accuracy: 0.6557\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 15s 7ms/step - loss: 0.4810 - accuracy: 0.7895 - val_loss: 0.7572 - val_accuracy: 0.6248\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4519 - accuracy: 0.8086 - val_loss: 0.6322 - val_accuracy: 0.6424\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4437 - accuracy: 0.8136 - val_loss: 0.7232 - val_accuracy: 0.6250\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4377 - accuracy: 0.8160 - val_loss: 0.5331 - val_accuracy: 0.6917\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4301 - accuracy: 0.8202 - val_loss: 0.7030 - val_accuracy: 0.6303\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4194 - accuracy: 0.8245 - val_loss: 0.6099 - val_accuracy: 0.6817\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4119 - accuracy: 0.8292 - val_loss: 0.5856 - val_accuracy: 0.6787\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4010 - accuracy: 0.8343 - val_loss: 0.5849 - val_accuracy: 0.6731\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3896 - accuracy: 0.8378 - val_loss: 0.6018 - val_accuracy: 0.6723\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3794 - accuracy: 0.8428 - val_loss: 0.6566 - val_accuracy: 0.6634\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.4929 - accuracy: 0.7875 - val_loss: 0.5218 - val_accuracy: 0.6810\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4629 - accuracy: 0.8047 - val_loss: 0.5225 - val_accuracy: 0.6715\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4552 - accuracy: 0.8066 - val_loss: 0.6173 - val_accuracy: 0.6503\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4495 - accuracy: 0.8109 - val_loss: 0.5013 - val_accuracy: 0.7051\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4452 - accuracy: 0.8132 - val_loss: 0.5249 - val_accuracy: 0.6837\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4391 - accuracy: 0.8154 - val_loss: 0.6447 - val_accuracy: 0.6408\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4327 - accuracy: 0.8167 - val_loss: 0.5451 - val_accuracy: 0.6773\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4278 - accuracy: 0.8204 - val_loss: 0.6545 - val_accuracy: 0.6655\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4209 - accuracy: 0.8241 - val_loss: 0.5803 - val_accuracy: 0.6710\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4144 - accuracy: 0.8283 - val_loss: 0.5676 - val_accuracy: 0.6762\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 14s 6ms/step - loss: 0.4816 - accuracy: 0.7910 - val_loss: 0.5842 - val_accuracy: 0.6857\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4496 - accuracy: 0.8109 - val_loss: 0.6423 - val_accuracy: 0.6622\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4401 - accuracy: 0.8154 - val_loss: 0.6029 - val_accuracy: 0.6530\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4314 - accuracy: 0.8193 - val_loss: 0.7107 - val_accuracy: 0.6149\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4200 - accuracy: 0.8261 - val_loss: 0.7245 - val_accuracy: 0.6457\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4080 - accuracy: 0.8315 - val_loss: 0.6110 - val_accuracy: 0.6649\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3929 - accuracy: 0.8388 - val_loss: 0.6065 - val_accuracy: 0.6851\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3773 - accuracy: 0.8450 - val_loss: 0.6364 - val_accuracy: 0.6829\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3609 - accuracy: 0.8528 - val_loss: 0.5346 - val_accuracy: 0.7067\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3482 - accuracy: 0.8571 - val_loss: 0.5565 - val_accuracy: 0.7121\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.4925 - accuracy: 0.7844 - val_loss: 0.5119 - val_accuracy: 0.7016\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4556 - accuracy: 0.8078 - val_loss: 0.5373 - val_accuracy: 0.7071\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4471 - accuracy: 0.8121 - val_loss: 0.5892 - val_accuracy: 0.6685\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4408 - accuracy: 0.8163 - val_loss: 0.6043 - val_accuracy: 0.6962\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4358 - accuracy: 0.8175 - val_loss: 0.5828 - val_accuracy: 0.6885\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4270 - accuracy: 0.8208 - val_loss: 0.5418 - val_accuracy: 0.7025\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4176 - accuracy: 0.8258 - val_loss: 0.6064 - val_accuracy: 0.6777\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4098 - accuracy: 0.8302 - val_loss: 0.5772 - val_accuracy: 0.6693\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3979 - accuracy: 0.8358 - val_loss: 0.5143 - val_accuracy: 0.7093\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3863 - accuracy: 0.8417 - val_loss: 0.5430 - val_accuracy: 0.6994\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.4836 - accuracy: 0.7897 - val_loss: 0.6322 - val_accuracy: 0.6518\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4578 - accuracy: 0.8059 - val_loss: 0.6632 - val_accuracy: 0.6380\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4501 - accuracy: 0.8114 - val_loss: 0.6079 - val_accuracy: 0.6765\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4458 - accuracy: 0.8117 - val_loss: 0.5833 - val_accuracy: 0.6658\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4396 - accuracy: 0.8150 - val_loss: 0.5629 - val_accuracy: 0.6869\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4337 - accuracy: 0.8173 - val_loss: 0.5608 - val_accuracy: 0.6768\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4275 - accuracy: 0.8200 - val_loss: 0.6216 - val_accuracy: 0.6621\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4195 - accuracy: 0.8242 - val_loss: 0.5792 - val_accuracy: 0.6840\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4139 - accuracy: 0.8269 - val_loss: 0.6481 - val_accuracy: 0.6697\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4071 - accuracy: 0.8287 - val_loss: 0.6631 - val_accuracy: 0.6567\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 14s 6ms/step - loss: 0.4978 - accuracy: 0.7823 - val_loss: 0.5174 - val_accuracy: 0.6767\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4677 - accuracy: 0.8030 - val_loss: 0.5914 - val_accuracy: 0.6445\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4605 - accuracy: 0.8061 - val_loss: 0.6091 - val_accuracy: 0.6528\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4551 - accuracy: 0.8084 - val_loss: 0.5894 - val_accuracy: 0.6755\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.4524 - accuracy: 0.8088 - val_loss: 0.6910 - val_accuracy: 0.6405\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4488 - accuracy: 0.8109 - val_loss: 0.6208 - val_accuracy: 0.6580\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4457 - accuracy: 0.8124 - val_loss: 0.6391 - val_accuracy: 0.6643\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4416 - accuracy: 0.8139 - val_loss: 0.6308 - val_accuracy: 0.6502\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.4375 - accuracy: 0.8138 - val_loss: 0.6565 - val_accuracy: 0.6467\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.4336 - accuracy: 0.8170 - val_loss: 0.6333 - val_accuracy: 0.6499\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 14s 6ms/step - loss: 0.4770 - accuracy: 0.7941 - val_loss: 0.6532 - val_accuracy: 0.6404\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4496 - accuracy: 0.8114 - val_loss: 0.5386 - val_accuracy: 0.6873\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4417 - accuracy: 0.8153 - val_loss: 0.6569 - val_accuracy: 0.6511\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4327 - accuracy: 0.8196 - val_loss: 0.5374 - val_accuracy: 0.7013\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4214 - accuracy: 0.8244 - val_loss: 0.6905 - val_accuracy: 0.6305\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4098 - accuracy: 0.8298 - val_loss: 0.5895 - val_accuracy: 0.6801\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3934 - accuracy: 0.8369 - val_loss: 0.6390 - val_accuracy: 0.6746\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3757 - accuracy: 0.8458 - val_loss: 0.5567 - val_accuracy: 0.7083\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3586 - accuracy: 0.8524 - val_loss: 0.5809 - val_accuracy: 0.6951\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3425 - accuracy: 0.8601 - val_loss: 0.6944 - val_accuracy: 0.6770\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 13s 6ms/step - loss: 0.4899 - accuracy: 0.7856 - val_loss: 0.7802 - val_accuracy: 0.6253\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4588 - accuracy: 0.8067 - val_loss: 0.7935 - val_accuracy: 0.6222\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4518 - accuracy: 0.8095 - val_loss: 0.5656 - val_accuracy: 0.6657\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4445 - accuracy: 0.8125 - val_loss: 0.6309 - val_accuracy: 0.6534\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4374 - accuracy: 0.8160 - val_loss: 0.5977 - val_accuracy: 0.6659\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4293 - accuracy: 0.8201 - val_loss: 0.6361 - val_accuracy: 0.6755\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.4208 - accuracy: 0.8242 - val_loss: 0.6049 - val_accuracy: 0.6814\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4091 - accuracy: 0.8296 - val_loss: 0.6427 - val_accuracy: 0.6756\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3998 - accuracy: 0.8340 - val_loss: 0.6965 - val_accuracy: 0.6467\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.3863 - accuracy: 0.8409 - val_loss: 0.6259 - val_accuracy: 0.6581\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 16s 7ms/step - loss: 0.4801 - accuracy: 0.7881 - val_loss: 0.5403 - val_accuracy: 0.7018\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4462 - accuracy: 0.8127 - val_loss: 0.5000 - val_accuracy: 0.6996\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4357 - accuracy: 0.8175 - val_loss: 0.6051 - val_accuracy: 0.6686\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4208 - accuracy: 0.8249 - val_loss: 0.8047 - val_accuracy: 0.6129\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.4007 - accuracy: 0.8330 - val_loss: 0.6258 - val_accuracy: 0.6796\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.3755 - accuracy: 0.8451 - val_loss: 0.4532 - val_accuracy: 0.7522\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.3465 - accuracy: 0.8578 - val_loss: 0.6581 - val_accuracy: 0.6748\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3166 - accuracy: 0.8695 - val_loss: 0.7244 - val_accuracy: 0.6778\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2876 - accuracy: 0.8824 - val_loss: 0.7578 - val_accuracy: 0.7003\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.2603 - accuracy: 0.8943 - val_loss: 0.7739 - val_accuracy: 0.6785\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 15s 7ms/step - loss: 0.4835 - accuracy: 0.7903 - val_loss: 0.6733 - val_accuracy: 0.6191\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.4537 - accuracy: 0.8100 - val_loss: 0.5687 - val_accuracy: 0.6799\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4436 - accuracy: 0.8134 - val_loss: 0.5085 - val_accuracy: 0.7086\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4346 - accuracy: 0.8183 - val_loss: 0.5403 - val_accuracy: 0.6657\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.4222 - accuracy: 0.8238 - val_loss: 0.6882 - val_accuracy: 0.6599\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4065 - accuracy: 0.8316 - val_loss: 0.4937 - val_accuracy: 0.7145\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3875 - accuracy: 0.8401 - val_loss: 0.6012 - val_accuracy: 0.6803\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3672 - accuracy: 0.8487 - val_loss: 0.5632 - val_accuracy: 0.7123\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 12s 7ms/step - loss: 0.3458 - accuracy: 0.8584 - val_loss: 0.6616 - val_accuracy: 0.6805\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 12s 6ms/step - loss: 0.3250 - accuracy: 0.8682 - val_loss: 0.6250 - val_accuracy: 0.6903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = process_data(test_data_dir)\n",
        "test_embd, skipped,  lines = load_pep_and_codify(processed_data['Peptide_seq'], 81)\n",
        "x_test = test_embd\n",
        "y_test = test_data['Class'].apply(lambda x: 1 if x == 'MObs' else 0).values\n",
        "x_test[0]\n",
        "y_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DJ-djux2Nnw7",
        "outputId": "6f3b5a42-1d5b-4afb-b5c3-1f879f2070ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AMGIMNSFVNDIFER\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "APQGijO2TIbP",
        "outputId": "d9ba4d4c-0951-4fb2-b85a-f1c62c726700"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Peptide_seq  BUNA790103  CHAM820102  CHOP780207  \\\n",
              "0              AMGIMNSFVNDIFER    6.286667    0.453200    0.962667   \n",
              "1                IQDKEGIPPDQQR    5.538462    0.279231    1.083077   \n",
              "2               LHFFMPGFAPLTSR    5.828571   -0.067929    1.109286   \n",
              "3             SYELPDGQVITIGNER    6.268750    0.590625    1.020625   \n",
              "4                    TAVCDIPPR    5.444444    0.211556    0.996667   \n",
              "...                        ...         ...         ...         ...   \n",
              "74993       VPPGLFNVVQGGAATGQF    6.005556   -0.081167    1.030000   \n",
              "74994         FIVVSMLSAIRGFSLE    6.581250    0.348438    0.840000   \n",
              "74995  FDPTWESLDARQLPAWFDQAKFG    5.786957    0.446478    0.920435   \n",
              "74996               VDGYIWSWTK    5.330000    1.031300    0.845000   \n",
              "74997           SQKLSPIYNLVPVK    5.735714    0.274714    1.035000   \n",
              "\n",
              "       FASG760102  FASG760103  GEIM800103  KARP850103  KHAG800101  LEWP710101  \\\n",
              "0      269.266667   -3.061333    1.046000    0.897133   31.613333    0.314667   \n",
              "1      239.076923   -6.115385    1.008462    0.918538   29.461538    0.370000   \n",
              "2      274.000000  -26.257143    0.757143    0.908500   49.107143    0.298571   \n",
              "3      265.750000   -4.376250    0.992500    0.910250   30.043750    0.355000   \n",
              "4      250.777778  -19.946667    0.846667    0.914778   38.788889    0.348889   \n",
              "...           ...         ...         ...         ...         ...         ...   \n",
              "74993  268.944444  -14.050556    0.770000    0.914556   40.283333    0.316667   \n",
              "74994  277.312500   -3.821250    0.970625    0.903875   39.006250    0.270625   \n",
              "74995  264.652174  -14.284783    1.070435    0.911609   38.443478    0.348696   \n",
              "74996  275.000000   -7.522000    1.161000    0.899700   32.940000    0.424000   \n",
              "74997  261.214286  -11.845714    0.950000    0.929714   21.564286    0.318571   \n",
              "\n",
              "       ...  ZASB820101  YUTK870104  AVBF000106  Aliphatic     Basic    Acidic  \\\n",
              "0      ...   -0.154333   17.190667   -0.238133   0.200000  0.066667  0.133333   \n",
              "1      ...   -0.195308   17.097692   -0.253909   0.153846  0.153846  0.230769   \n",
              "2      ...   -0.105286   17.102857   -0.274917   0.142857  0.142857  0.000000   \n",
              "3      ...   -0.186438   17.309375   -0.238400   0.250000  0.062500  0.187500   \n",
              "4      ...   -0.148778   16.420000   -0.196714   0.222222  0.111111  0.111111   \n",
              "...    ...         ...         ...         ...        ...       ...       ...   \n",
              "74993  ...   -0.146444   18.542222   -0.300375   0.222222  0.000000  0.000000   \n",
              "74994  ...   -0.136375   17.379375   -0.239187   0.375000  0.062500  0.062500   \n",
              "74995  ...   -0.131348   17.406087   -0.252143   0.086957  0.086957  0.173913   \n",
              "74996  ...   -0.064100   18.080000   -0.209300   0.200000  0.100000  0.100000   \n",
              "74997  ...   -0.129714   18.610714   -0.224000   0.357143  0.142857  0.000000   \n",
              "\n",
              "       pep_length    pep_PI     pep_PMW  Class  \n",
              "0              15  4.370356  1744.00032   MObs  \n",
              "1              13  4.558133  1523.64664   MObs  \n",
              "2              14  9.756550  1620.91392   MObs  \n",
              "3              16  4.136799  1790.92442   MObs  \n",
              "4               9  5.496877   971.13210   MObs  \n",
              "...           ...       ...         ...    ...  \n",
              "74993          18  5.494998  1758.97022   LObs  \n",
              "74994          16  6.218211  1769.11252   LObs  \n",
              "74995          23  4.225967  2725.96108   LObs  \n",
              "74996          10  5.805261  1254.38922   LObs  \n",
              "74997          14  9.700074  1585.88496   LObs  \n",
              "\n",
              "[74998 rows x 59 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3259650-c4cc-4902-a17a-15afabc22f1b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Peptide_seq</th>\n",
              "      <th>BUNA790103</th>\n",
              "      <th>CHAM820102</th>\n",
              "      <th>CHOP780207</th>\n",
              "      <th>FASG760102</th>\n",
              "      <th>FASG760103</th>\n",
              "      <th>GEIM800103</th>\n",
              "      <th>KARP850103</th>\n",
              "      <th>KHAG800101</th>\n",
              "      <th>LEWP710101</th>\n",
              "      <th>...</th>\n",
              "      <th>ZASB820101</th>\n",
              "      <th>YUTK870104</th>\n",
              "      <th>AVBF000106</th>\n",
              "      <th>Aliphatic</th>\n",
              "      <th>Basic</th>\n",
              "      <th>Acidic</th>\n",
              "      <th>pep_length</th>\n",
              "      <th>pep_PI</th>\n",
              "      <th>pep_PMW</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AMGIMNSFVNDIFER</td>\n",
              "      <td>6.286667</td>\n",
              "      <td>0.453200</td>\n",
              "      <td>0.962667</td>\n",
              "      <td>269.266667</td>\n",
              "      <td>-3.061333</td>\n",
              "      <td>1.046000</td>\n",
              "      <td>0.897133</td>\n",
              "      <td>31.613333</td>\n",
              "      <td>0.314667</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.154333</td>\n",
              "      <td>17.190667</td>\n",
              "      <td>-0.238133</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>15</td>\n",
              "      <td>4.370356</td>\n",
              "      <td>1744.00032</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IQDKEGIPPDQQR</td>\n",
              "      <td>5.538462</td>\n",
              "      <td>0.279231</td>\n",
              "      <td>1.083077</td>\n",
              "      <td>239.076923</td>\n",
              "      <td>-6.115385</td>\n",
              "      <td>1.008462</td>\n",
              "      <td>0.918538</td>\n",
              "      <td>29.461538</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195308</td>\n",
              "      <td>17.097692</td>\n",
              "      <td>-0.253909</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>13</td>\n",
              "      <td>4.558133</td>\n",
              "      <td>1523.64664</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LHFFMPGFAPLTSR</td>\n",
              "      <td>5.828571</td>\n",
              "      <td>-0.067929</td>\n",
              "      <td>1.109286</td>\n",
              "      <td>274.000000</td>\n",
              "      <td>-26.257143</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.908500</td>\n",
              "      <td>49.107143</td>\n",
              "      <td>0.298571</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.105286</td>\n",
              "      <td>17.102857</td>\n",
              "      <td>-0.274917</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>9.756550</td>\n",
              "      <td>1620.91392</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SYELPDGQVITIGNER</td>\n",
              "      <td>6.268750</td>\n",
              "      <td>0.590625</td>\n",
              "      <td>1.020625</td>\n",
              "      <td>265.750000</td>\n",
              "      <td>-4.376250</td>\n",
              "      <td>0.992500</td>\n",
              "      <td>0.910250</td>\n",
              "      <td>30.043750</td>\n",
              "      <td>0.355000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.186438</td>\n",
              "      <td>17.309375</td>\n",
              "      <td>-0.238400</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>16</td>\n",
              "      <td>4.136799</td>\n",
              "      <td>1790.92442</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TAVCDIPPR</td>\n",
              "      <td>5.444444</td>\n",
              "      <td>0.211556</td>\n",
              "      <td>0.996667</td>\n",
              "      <td>250.777778</td>\n",
              "      <td>-19.946667</td>\n",
              "      <td>0.846667</td>\n",
              "      <td>0.914778</td>\n",
              "      <td>38.788889</td>\n",
              "      <td>0.348889</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.148778</td>\n",
              "      <td>16.420000</td>\n",
              "      <td>-0.196714</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>9</td>\n",
              "      <td>5.496877</td>\n",
              "      <td>971.13210</td>\n",
              "      <td>MObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74993</th>\n",
              "      <td>VPPGLFNVVQGGAATGQF</td>\n",
              "      <td>6.005556</td>\n",
              "      <td>-0.081167</td>\n",
              "      <td>1.030000</td>\n",
              "      <td>268.944444</td>\n",
              "      <td>-14.050556</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.914556</td>\n",
              "      <td>40.283333</td>\n",
              "      <td>0.316667</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.146444</td>\n",
              "      <td>18.542222</td>\n",
              "      <td>-0.300375</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18</td>\n",
              "      <td>5.494998</td>\n",
              "      <td>1758.97022</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74994</th>\n",
              "      <td>FIVVSMLSAIRGFSLE</td>\n",
              "      <td>6.581250</td>\n",
              "      <td>0.348438</td>\n",
              "      <td>0.840000</td>\n",
              "      <td>277.312500</td>\n",
              "      <td>-3.821250</td>\n",
              "      <td>0.970625</td>\n",
              "      <td>0.903875</td>\n",
              "      <td>39.006250</td>\n",
              "      <td>0.270625</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.136375</td>\n",
              "      <td>17.379375</td>\n",
              "      <td>-0.239187</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>16</td>\n",
              "      <td>6.218211</td>\n",
              "      <td>1769.11252</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74995</th>\n",
              "      <td>FDPTWESLDARQLPAWFDQAKFG</td>\n",
              "      <td>5.786957</td>\n",
              "      <td>0.446478</td>\n",
              "      <td>0.920435</td>\n",
              "      <td>264.652174</td>\n",
              "      <td>-14.284783</td>\n",
              "      <td>1.070435</td>\n",
              "      <td>0.911609</td>\n",
              "      <td>38.443478</td>\n",
              "      <td>0.348696</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.131348</td>\n",
              "      <td>17.406087</td>\n",
              "      <td>-0.252143</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>23</td>\n",
              "      <td>4.225967</td>\n",
              "      <td>2725.96108</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74996</th>\n",
              "      <td>VDGYIWSWTK</td>\n",
              "      <td>5.330000</td>\n",
              "      <td>1.031300</td>\n",
              "      <td>0.845000</td>\n",
              "      <td>275.000000</td>\n",
              "      <td>-7.522000</td>\n",
              "      <td>1.161000</td>\n",
              "      <td>0.899700</td>\n",
              "      <td>32.940000</td>\n",
              "      <td>0.424000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064100</td>\n",
              "      <td>18.080000</td>\n",
              "      <td>-0.209300</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>10</td>\n",
              "      <td>5.805261</td>\n",
              "      <td>1254.38922</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74997</th>\n",
              "      <td>SQKLSPIYNLVPVK</td>\n",
              "      <td>5.735714</td>\n",
              "      <td>0.274714</td>\n",
              "      <td>1.035000</td>\n",
              "      <td>261.214286</td>\n",
              "      <td>-11.845714</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.929714</td>\n",
              "      <td>21.564286</td>\n",
              "      <td>0.318571</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.129714</td>\n",
              "      <td>18.610714</td>\n",
              "      <td>-0.224000</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14</td>\n",
              "      <td>9.700074</td>\n",
              "      <td>1585.88496</td>\n",
              "      <td>LObs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>74998 rows Ã— 59 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3259650-c4cc-4902-a17a-15afabc22f1b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f3259650-c4cc-4902-a17a-15afabc22f1b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f3259650-c4cc-4902-a17a-15afabc22f1b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c7c5d6bd-8aa9-4762-b8c0-0578db23351f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c7c5d6bd-8aa9-4762-b8c0-0578db23351f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c7c5d6bd-8aa9-4762-b8c0-0578db23351f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_data"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "### Metrics Descriptions\n",
        "The primary metrics used to evaluate the model are:\n",
        "\n",
        "- Accuracy: Measures the proportion of correctly predicted observations to the total observations..\n",
        "- F1-Score: The weighted average of Precision and Recall. Useful in cases of class imbalance.\n",
        "- Precision: Measures the proportion of correctly predicted positive observations from the total predicted positives.\n",
        "- Recall (Sensitivity): Measures the proportion of actual positives that were correctly identified."
      ],
      "metadata": {
        "id": "bi12dauQZGL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best_acc = 0\n",
        "# best_model = model_list[0]\n",
        "# for m in model_list:\n",
        "#   loss, accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "#   print(f\"Test Loss: {loss:.4f}\")\n",
        "#   print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "#   if accuracy > best_acc:\n",
        "#     best_acc = accuracy\n",
        "#     best_model = m\n",
        "\n",
        "# print('best model found,' + 'accuracy:' + str(best_acc))\n",
        "# best_model.summary()\n",
        "\n",
        "for m in model_list:\n",
        "  loss, accuracy = m.evaluate(x_test, y_test)\n",
        "\n",
        "  print(f\"Test Loss: {loss:.4f}\")\n",
        "  print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "64PU_wLaNQwu",
        "outputId": "9b7d8e3f-fb37-4736-987e-90de386974ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2344/2344 [==============================] - 9s 4ms/step - loss: 0.4789 - accuracy: 0.7842\n",
            "Test Loss: 0.4789\n",
            "Test Accuracy: 0.7842\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.5024 - accuracy: 0.7800\n",
            "Test Loss: 0.5024\n",
            "Test Accuracy: 0.7800\n",
            "2344/2344 [==============================] - 5s 2ms/step - loss: 0.4525 - accuracy: 0.7989\n",
            "Test Loss: 0.4525\n",
            "Test Accuracy: 0.7989\n",
            "2344/2344 [==============================] - 9s 4ms/step - loss: 0.4870 - accuracy: 0.7838\n",
            "Test Loss: 0.4870\n",
            "Test Accuracy: 0.7838\n",
            "2344/2344 [==============================] - 5s 2ms/step - loss: 0.4435 - accuracy: 0.8018\n",
            "Test Loss: 0.4435\n",
            "Test Accuracy: 0.8018\n",
            "2344/2344 [==============================] - 5s 2ms/step - loss: 0.4723 - accuracy: 0.7968\n",
            "Test Loss: 0.4723\n",
            "Test Accuracy: 0.7968\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.4436 - accuracy: 0.7964\n",
            "Test Loss: 0.4436\n",
            "Test Accuracy: 0.7964\n",
            "2344/2344 [==============================] - 5s 2ms/step - loss: 0.4691 - accuracy: 0.7875\n",
            "Test Loss: 0.4691\n",
            "Test Accuracy: 0.7875\n",
            "2344/2344 [==============================] - 6s 2ms/step - loss: 0.4079 - accuracy: 0.8202\n",
            "Test Loss: 0.4079\n",
            "Test Accuracy: 0.8202\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.4346 - accuracy: 0.8077\n",
            "Test Loss: 0.4346\n",
            "Test Accuracy: 0.8077\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.3506 - accuracy: 0.8504\n",
            "Test Loss: 0.3506\n",
            "Test Accuracy: 0.8504\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.3902 - accuracy: 0.8305\n",
            "Test Loss: 0.3902\n",
            "Test Accuracy: 0.8305\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.4339 - accuracy: 0.8073\n",
            "Test Loss: 0.4339\n",
            "Test Accuracy: 0.8073\n",
            "2344/2344 [==============================] - 8s 4ms/step - loss: 0.4602 - accuracy: 0.7920\n",
            "Test Loss: 0.4602\n",
            "Test Accuracy: 0.7920\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.3674 - accuracy: 0.8428\n",
            "Test Loss: 0.3674\n",
            "Test Accuracy: 0.8428\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.4070 - accuracy: 0.8187\n",
            "Test Loss: 0.4070\n",
            "Test Accuracy: 0.8187\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.3016 - accuracy: 0.8801\n",
            "Test Loss: 0.3016\n",
            "Test Accuracy: 0.8801\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.3375 - accuracy: 0.8570\n",
            "Test Loss: 0.3375\n",
            "Test Accuracy: 0.8570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# Initialize variables to keep track of the best model\n",
        "best_f1_score = 0\n",
        "best_model = None\n",
        "best_model_index = -1\n",
        "model_index = 0\n",
        "\n",
        "# List to store reports for each model\n",
        "model_reports = []\n",
        "\n",
        "# Loop through each model in the list\n",
        "for model in model_list:\n",
        "    # Make predictions with the current model\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_classes = (y_pred >= 0.5).astype(\"int32\")  # Binary classification\n",
        "\n",
        "    # Generate and store the classification report\n",
        "    report_dict = classification_report(y_test, y_pred_classes, target_names=[\"Class 0\", \"Class 1\"], output_dict=True)\n",
        "    model_reports.append(report_dict)\n",
        "\n",
        "    # Calculate the F1-score for the current model (considering the weighted average if it's a balanced consideration)\n",
        "    current_f1_score = report_dict['weighted avg']['f1-score']\n",
        "\n",
        "    print(f\"Model {model_index}:\")\n",
        "    print(classification_report(y_test, y_pred_classes, target_names=[\"Class 0\", \"Class 1\"]))\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Check if the current model's F1-score is the best and update the best model tracking variables\n",
        "    if current_f1_score > best_f1_score:\n",
        "        best_f1_score = current_f1_score\n",
        "        best_model = model\n",
        "        best_model_index = model_index\n",
        "\n",
        "    model_index += 1\n",
        "\n",
        "# Output the best model's performance\n",
        "print(f\"The best model is Model {best_model_index} with an F1-score of {best_f1_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "xI8gO3N-U__v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b318184e-d89a-499c-feb9-3bceff3096ab"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2344/2344 [==============================] - 7s 3ms/step\n",
            "Model 0:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.90      0.64      0.75     37500\n",
            "     Class 1       0.72      0.93      0.81     37498\n",
            "\n",
            "    accuracy                           0.78     74998\n",
            "   macro avg       0.81      0.78      0.78     74998\n",
            "weighted avg       0.81      0.78      0.78     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[23861 13639]\n",
            " [ 2548 34950]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.89      0.64      0.74     37500\n",
            "     Class 1       0.72      0.92      0.81     37498\n",
            "\n",
            "    accuracy                           0.78     74998\n",
            "   macro avg       0.81      0.78      0.78     74998\n",
            "weighted avg       0.81      0.78      0.78     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[23832 13668]\n",
            " [ 2831 34667]]\n",
            "2344/2344 [==============================] - 5s 2ms/step\n",
            "Model 2:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.90      0.67      0.77     37500\n",
            "     Class 1       0.74      0.92      0.82     37498\n",
            "\n",
            "    accuracy                           0.80     74998\n",
            "   macro avg       0.82      0.80      0.80     74998\n",
            "weighted avg       0.82      0.80      0.80     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[25257 12243]\n",
            " [ 2840 34658]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 3:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.91      0.63      0.75     37500\n",
            "     Class 1       0.72      0.94      0.81     37498\n",
            "\n",
            "    accuracy                           0.78     74998\n",
            "   macro avg       0.81      0.78      0.78     74998\n",
            "weighted avg       0.81      0.78      0.78     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[23703 13797]\n",
            " [ 2418 35080]]\n",
            "2344/2344 [==============================] - 5s 2ms/step\n",
            "Model 4:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.91      0.67      0.77     37500\n",
            "     Class 1       0.74      0.93      0.82     37498\n",
            "\n",
            "    accuracy                           0.80     74998\n",
            "   macro avg       0.82      0.80      0.80     74998\n",
            "weighted avg       0.82      0.80      0.80     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[25140 12360]\n",
            " [ 2501 34997]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 5:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.89      0.68      0.77     37500\n",
            "     Class 1       0.74      0.91      0.82     37498\n",
            "\n",
            "    accuracy                           0.80     74998\n",
            "   macro avg       0.81      0.80      0.79     74998\n",
            "weighted avg       0.81      0.80      0.79     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[25564 11936]\n",
            " [ 3302 34196]]\n",
            "2344/2344 [==============================] - 5s 2ms/step\n",
            "Model 6:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.91      0.66      0.76     37500\n",
            "     Class 1       0.73      0.93      0.82     37498\n",
            "\n",
            "    accuracy                           0.80     74998\n",
            "   macro avg       0.82      0.80      0.79     74998\n",
            "weighted avg       0.82      0.80      0.79     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[24712 12788]\n",
            " [ 2484 35014]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 7:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.91      0.64      0.75     37500\n",
            "     Class 1       0.72      0.93      0.81     37498\n",
            "\n",
            "    accuracy                           0.79     74998\n",
            "   macro avg       0.81      0.79      0.78     74998\n",
            "weighted avg       0.81      0.79      0.78     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[24032 13468]\n",
            " [ 2468 35030]]\n",
            "2344/2344 [==============================] - 5s 2ms/step\n",
            "Model 8:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.95      0.68      0.79     37500\n",
            "     Class 1       0.75      0.96      0.84     37498\n",
            "\n",
            "    accuracy                           0.82     74998\n",
            "   macro avg       0.85      0.82      0.82     74998\n",
            "weighted avg       0.85      0.82      0.82     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[25420 12080]\n",
            " [ 1406 36092]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 9:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.92      0.68      0.78     37500\n",
            "     Class 1       0.74      0.94      0.83     37498\n",
            "\n",
            "    accuracy                           0.81     74998\n",
            "   macro avg       0.83      0.81      0.80     74998\n",
            "weighted avg       0.83      0.81      0.80     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[25407 12093]\n",
            " [ 2329 35169]]\n",
            "2344/2344 [==============================] - 5s 2ms/step\n",
            "Model 10:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.94      0.75      0.83     37500\n",
            "     Class 1       0.79      0.95      0.86     37498\n",
            "\n",
            "    accuracy                           0.85     74998\n",
            "   macro avg       0.87      0.85      0.85     74998\n",
            "weighted avg       0.87      0.85      0.85     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[28095  9405]\n",
            " [ 1813 35685]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 11:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.93      0.72      0.81     37500\n",
            "     Class 1       0.77      0.94      0.85     37498\n",
            "\n",
            "    accuracy                           0.83     74998\n",
            "   macro avg       0.85      0.83      0.83     74998\n",
            "weighted avg       0.85      0.83      0.83     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26945 10555]\n",
            " [ 2154 35344]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 12:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.93      0.66      0.77     37500\n",
            "     Class 1       0.74      0.95      0.83     37498\n",
            "\n",
            "    accuracy                           0.81     74998\n",
            "   macro avg       0.84      0.81      0.80     74998\n",
            "weighted avg       0.84      0.81      0.80     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[24771 12729]\n",
            " [ 1726 35772]]\n",
            "2344/2344 [==============================] - 5s 2ms/step\n",
            "Model 13:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.92      0.64      0.75     37500\n",
            "     Class 1       0.72      0.95      0.82     37498\n",
            "\n",
            "    accuracy                           0.79     74998\n",
            "   macro avg       0.82      0.79      0.79     74998\n",
            "weighted avg       0.82      0.79      0.79     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[23942 13558]\n",
            " [ 2043 35455]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 14:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.96      0.71      0.82     37500\n",
            "     Class 1       0.77      0.97      0.86     37498\n",
            "\n",
            "    accuracy                           0.84     74998\n",
            "   macro avg       0.87      0.84      0.84     74998\n",
            "weighted avg       0.87      0.84      0.84     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26692 10808]\n",
            " [  978 36520]]\n",
            "2344/2344 [==============================] - 5s 2ms/step\n",
            "Model 15:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.95      0.67      0.79     37500\n",
            "     Class 1       0.75      0.96      0.84     37498\n",
            "\n",
            "    accuracy                           0.82     74998\n",
            "   macro avg       0.85      0.82      0.81     74998\n",
            "weighted avg       0.85      0.82      0.81     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[25217 12283]\n",
            " [ 1316 36182]]\n",
            "2344/2344 [==============================] - 4s 2ms/step\n",
            "Model 16:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.98      0.77      0.87     37500\n",
            "     Class 1       0.81      0.99      0.89     37498\n",
            "\n",
            "    accuracy                           0.88     74998\n",
            "   macro avg       0.90      0.88      0.88     74998\n",
            "weighted avg       0.90      0.88      0.88     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[28965  8535]\n",
            " [  461 37037]]\n",
            "2344/2344 [==============================] - 6s 2ms/step\n",
            "Model 17:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.96      0.74      0.84     37500\n",
            "     Class 1       0.79      0.97      0.87     37498\n",
            "\n",
            "    accuracy                           0.86     74998\n",
            "   macro avg       0.88      0.86      0.86     74998\n",
            "weighted avg       0.88      0.86      0.86     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[27838  9662]\n",
            " [ 1066 36432]]\n",
            "The best model is Model 16 with an F1-score of 0.8786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GviCKL_6upbQ",
        "outputId": "94b4b90c-0777-4a77-f6e2-7594e1a3cc89"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_30 (Embedding)    (None, 81, 50)            1150      \n",
            "                                                                 \n",
            " conv1d_60 (Conv1D)          (None, 81, 256)           89856     \n",
            "                                                                 \n",
            " conv1d_61 (Conv1D)          (None, 81, 128)           229504    \n",
            "                                                                 \n",
            " global_max_pooling1d_30 (G  (None, 128)               0         \n",
            " lobalMaxPooling1D)                                              \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 328831 (1.25 MB)\n",
            "Trainable params: 328831 (1.25 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Predicting the labels for the test set\n",
        "y_pred = best_model.predict(x_test)\n",
        "y_pred_classes = (y_pred >= 0.5).astype(\"int32\")  # Assuming binary classification with a sigmoid output\n",
        "\n",
        "# Generate a classification report\n",
        "report_dict = classification_report(y_test, y_pred_classes, target_names=[\"Class 0\", \"Class 1\"], output_dict=True)\n",
        "print(classification_report(y_test, y_pred_classes, target_names=[\"Class 0\", \"Class 1\"]))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "niLDoMoAWl0M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d5c85a04-db3f-489d-e11d-97e963f84996"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2344/2344 [==============================] - 5s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.98      0.77      0.87     37500\n",
            "     Class 1       0.81      0.99      0.89     37498\n",
            "\n",
            "    accuracy                           0.88     74998\n",
            "   macro avg       0.90      0.88      0.88     74998\n",
            "weighted avg       0.90      0.88      0.88     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[28965  8535]\n",
            " [  461 37037]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the F1-score for each class\n",
        "f1_score_class_0 = report_dict[\"Class 0\"][\"f1-score\"]\n",
        "f1_score_class_1 = report_dict[\"Class 1\"][\"f1-score\"]\n",
        "f1_score_average = report_dict[\"weighted avg\"][\"f1-score\"]\n",
        "\n",
        "print(f\"F1-score for Class 0: {f1_score_class_0:.2f}\")\n",
        "print(f\"F1-score for Class 1: {f1_score_class_1:.2f}\")\n",
        "print(f\"Weighted average F1-score: {f1_score_average:.2f}\")"
      ],
      "metadata": {
        "id": "T2_7rZrOWqVS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "56ecbd65-aed1-45dc-f4e1-d08d4f6b5bf4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score for Class 0: 0.87\n",
            "F1-score for Class 1: 0.89\n",
            "Weighted average F1-score: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LPpIbDC1rD9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the old version code without hyperparameter turning, saved for ablation study.\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# # Predicting the labels for the test set\n",
        "# y_pred = model.predict(x_test)\n",
        "# y_pred_classes = (y_pred >= 0.5).astype(\"int32\")  # Assuming binary classification with a sigmoid output\n",
        "\n",
        "# # Generate a classification report\n",
        "# report_dict = classification_report(y_test, y_pred_classes, target_names=[\"Class 0\", \"Class 1\"], output_dict=True)\n",
        "# print(classification_report(y_test, y_pred_classes, target_names=[\"Class 0\", \"Class 1\"]))\n",
        "\n",
        "# # Confusion matrix\n",
        "# conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNPcva3pOnRQ",
        "outputId": "3baed847-e03b-414f-bf81-90dd30e308ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2344/2344 [==============================] - 21s 9ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.90      0.67      0.76     37500\n",
            "     Class 1       0.73      0.92      0.82     37498\n",
            "\n",
            "    accuracy                           0.79     74998\n",
            "   macro avg       0.82      0.79      0.79     74998\n",
            "weighted avg       0.82      0.79      0.79     74998\n",
            "\n",
            "Confusion Matrix:\n",
            "[[25015 12485]\n",
            " [ 2916 34582]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the old version code without hyperparameter turning, saved for ablation study.\n",
        "# Accessing the F1-score for each class\n",
        "# f1_score_class_0 = report_dict[\"Class 0\"][\"f1-score\"]\n",
        "# f1_score_class_1 = report_dict[\"Class 1\"][\"f1-score\"]\n",
        "# f1_score_average = report_dict[\"weighted avg\"][\"f1-score\"]\n",
        "\n",
        "# print(f\"F1-score for Class 0: {f1_score_class_0:.2f}\")\n",
        "# print(f\"F1-score for Class 1: {f1_score_class_1:.2f}\")\n",
        "# print(f\"Weighted average F1-score: {f1_score_average:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj1QuORoQzE_",
        "outputId": "de280ce1-8efd-4304-81f8-94e27931e1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score for Class 0: 0.76\n",
            "F1-score for Class 1: 0.82\n",
            "Weighted average F1-score: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "The results from training and testing the 1D-2C-CNN model on the peptide detectability dataset are presented in this section. The model was evaluated on the test dataset after completing the training process, and the performance metrics were recorded.\n",
        "\n",
        "## Model Performance\n",
        "\n",
        "The model achieved the following results on the test dataset:\n",
        "\n",
        "- **Test Loss**: 0.3016\n",
        "- **Test Accuracy**: 88.01%\n",
        "\n",
        "These metrics indicate that the model was able to accurately predict peptide detectability with a reasonably high level of accuracy.\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_pred_prob is the probability output of the model for class 1\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "NqszziMKWox3",
        "outputId": "11defd5c-7d2d-4d8e-def9-a60dcf04fe62"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison\n",
        "\n",
        "Here we can see the performance of our initial model is very close to the best model devised in the paper. After we implement hyperparameter tuning process, our best model outperformed initial model and also similar model in original paper with 0.88 accuracy and 0.8786 F1-score."
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model         | Accuracy | F-score |\n",
        "|---------------|----------|---------|\n",
        "| Our Model     | 0.8801   | 0.8786  |\n",
        "| Our Model(Before tuning)     | 0.7946   | 0.7900  |\n",
        "| 1D-2C-CNN     | 0.7953   | 0.7744  |\n",
        "| 1D-1C-CNN     | 0.7917   | 0.7638  |\n",
        "| RFa           | 0.6924   | 0.6649  |\n",
        "| SvmRa         | 0.6813   | 0.6453  |\n",
        "| DNNb          | 0.6692   | 0.6659  |\n",
        "| C5a           | 0.6644   | 0.6687  |\n",
        "| Nneta         | 0.6723   | 0.6097  |\n",
        "| Rparta        | 0.6527   | 0.6167  |\n",
        "| Nba           | 0.5997   | 0.5408  |\n",
        "| Plsa          | 0.6043   | 0.5898  |\n",
        "| Glma          | 0.6036   | 0.5875  |\n",
        "| GlmStepAICa   | 0.6034   | 0.5874  |\n",
        "| Gaussianc     | 0.5983   | 0.5927  |\n",
        "| Jripa         | 0.6240   | 0.6119  |\n"
      ],
      "metadata": {
        "id": "-ZbPVWyqU3iQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyses\n",
        "The model demonstrates a stronger ability to identify 'detectable' peptides (Class 1) with higher recall, which suggests it is particularly effective in minimizing false negativesâ€”a critical aspect in proteomics where missing a detectable peptide could be more detrimental than falsely identifying one.\n",
        "\n",
        "The precision for 'non-detectable' peptides (Class 0) is notably high, though the recall is lower, indicating that while the model is reliable when it predicts a peptide as non-detectable, it tends to misclassify a fair number of non-detectable peptides as detectable.\n",
        "\n",
        "The marked improvement from our initial model to the final model underscores the critical role of fine-tuning in optimizing performance for this type of model."
      ],
      "metadata": {
        "id": "HtsKaHcRX4NA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Hypothesis assessment\n",
        "\n",
        "In reviewing the hypotheses laid out in the original study and the outcomes from our experimental evaluations, it becomes evident that our reproduction and enhancement efforts have yielded significant findings.\n",
        "\n",
        "### Hypothesis 1&2: Comparable and State of the Art\n",
        "\n",
        "The original paper hypothesized that the 1D-2C-CNN model would surpass traditional models, which rely on physicochemical properties of peptides, in terms of accuracy and precision. The original results reported an accuracy of 0.7953 and an F1-score of 0.7744 for the 1D-2C-CNN. Our experiments, which retained the model architecture but included additional hyperparameter tuning, have exceeded these benchmarks, achieving an accuracy of 0.8801 and an F1-score of 0.8786. These results not only confirm that the model outperforms traditional approaches but also suggest that its performance can be further enhanced through optimization, without necessitating architectural modifications.\n",
        "\n",
        "\n",
        "## Hypothesis 3: Scalability and Efficiency\n",
        "\n",
        "Finally, the original study hypothesized that the 1D-2C-CNN model would remain efficient and scalable even under limited computational resources. Our Experiment 3 tested this by monitoring the resources required during training and deployment. For example, the training time per model was approximately 2 minutes using free Colab access with a T4 GPU, and the entire tuning process took less than 40 minutes, this efficiency is partly attributable to the typically small dataset size in this field. However, the quick processing times are both practical and favorable for this type of analysis. Which indicates that the model is relatively resource-efficient.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8lRgeLiNP6_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments beyond the original paper and Ablation Study\n",
        "\n",
        "The architecture of the original model is straightforward, which allows us to concisely discuss the results of our tuning efforts and their impact.\n",
        "\n",
        "### Experiments - hyperparamenter tuning\n",
        "We further refined the model through hyperparameter tuning, achieving significantly improved performance with specific parameter settings. This enhanced model demonstrates the potential gains from meticulous optimization.\n",
        "\n",
        "### Ablation\n",
        "We retained the initial version of our model for comparison purposes. Interestingly, its performance was already comparable to that reported in the original paper even without any tuning. However, the substantial improvements achieved through the tuning process underscore its critical role. These enhancements are not only significant but also indispensable, highlighting the value of fine-tuning in achieving optimal model performance."
      ],
      "metadata": {
        "id": "e3B0fwP8TDmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Implications of Experimental Results\n",
        "The results of our experiments have significant implications for the field of proteomic analysis using machine learning. By surpassing the benchmarks set in the original paper through hyperparameter tuning, our study demonstrates that deep learning models, specifically 1D-2C-CNNs, can be highly effective for predicting peptide detectability. This advancement suggests that such models may replace or complement traditional methods that rely on physicochemical properties, thereby streamlining analytical workflows in proteomics.\n",
        "\n",
        "## Reproducibility of the Original Paper\n",
        "Our experiments confirmed that the fundamental findings of the original paper are reproducible, as we could achieve and even exceed the performance metrics reported with similar model architecture. However, achieving these results required additional tuning and optimization, which were not fully detailed in the original study. This indicates that while the core claims are valid, the reproducibility of the exact performance figures depends significantly on the precise setup and computational environment.\n",
        "\n",
        "## What Was Easy\n",
        "\n",
        "  * Model Implementation: The architecture of the 1D-2C-CNN was straightforward to implement with modern deep learning frameworks like TensorFlow and Keras.\n",
        "  * Initial Results Reproduction: Reproducing the initial results without tuning was relatively straightforward, which helped in establishing a baseline for further experiments.\n",
        "\n",
        "## What Was Difficult\n",
        "\n",
        "  * Hyperparameter Optimization: Determining the optimal settings for hyperparameters required extensive experimentation and was time-consuming.\n",
        "\n",
        "## Recommendations for Improving Reproducibility\n",
        "  * Detailed Documentation: Future papers should include comprehensive documentation of all model parameters, training procedures, and data preprocessing steps. This will aid researchers in replicating the results without ambiguity.\n",
        "  * Code Sharing: Sharing the complete codebase can significantly enhance reproducibility. While we compare the performance between models, we are also interested to check the model code and parameters set which this paper did not provide.\n",
        "\n",
        "##Next Steps\n",
        "In the next phase of our research, we plan to:\n",
        "\n",
        "  * Explore Additional Architectural Features: Integrate advanced neural network features such as attention mechanisms or residual connections to see if they can further improve the modelâ€™s performance.\n",
        "  * Expand Dataset Coverage: To test the modelâ€™s robustness and generalizability, we will apply it to a broader set of peptide sequences from various sources.\n",
        "  * Automated Hyperparameter Optimization: Implement techniques like grid search or Bayesian optimization to systematically explore hyperparameter spaces, aiming to further enhance model performance without extensive manual tuning.\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "  * Serrano, G, [DeepMSPeptide: peptide detectability prediction using deep learning], [Bioinformatics], [2020], [Volume 36]:[Issue 4], doi: https://doi.org/10.1093/bioinformatics/btz708\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}